{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML - Final Assigment\n",
    "\n",
    "Yael Ohayon - 312542558"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Mission - Clustring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Use one or more of the clustering algorithms we discussed \n",
    "in class to cluster together artists based on similarities.\n",
    "Usually, we use unsupervised learning in the earlier stages of the project.\n",
    "Discuss the results and support your claim in at least one plot (in addition to the clustering plot)\n",
    "This graph may relate to your predictions or incorporate any information from an outside source\n",
    "(please mention explicitly any source you used as your help).</b>\n",
    "\n",
    "In the follwing pargraph I will explain what I did in order to clustering the data\n",
    "\n",
    "<i> Stage I: Pre-Processing: </i>\n",
    "\n",
    "As also explained in requirements I did some pre-processing which is neccery to analyze the data later.\n",
    "For begining I used loop iteration over all images given in train and validation folders and using the python package for image processing - openCV,I resized all images to 100 * 100 size, so from now on - all the analyze made over picture of same dimension.The function I used can be found in this notebook at last.\n",
    " \n",
    "Later In order to answer the question above I choose to cluster the data according to the given label of artists origin\n",
    "\n",
    "<i> How I did it?</i>\n",
    "\n",
    "First because the requirements asked to plot graphs - I knew I have to use 2 or 3 dimensions - because more than that is not possible for plotting.\n",
    "So - I needed to reduce the image direction, which although was minimized to 100*100*3 (because each pixel holds 3 color)\n",
    "It still alot!\n",
    "\n",
    "I read about image processing and dimension reduction and desided to use PCA (Principal Componenet Analysis) in order to reduce image dimension.\n",
    "\n",
    "<i> Why PCA? </i>\n",
    "\n",
    "PCA create a new axis - which is linear combination of other features that explain most of data variance .\n",
    "Mathematiclly we search after the combintaion that project over it, as new axis, will give us the highest variacne of original data compare to all other projection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pay attention - Due to the high cost (in time and memory) of processing the data I created all tables as numpy or pandas object (pickles) that are already in this folder and loaded for anlyzing (but not created here) - in order to check the code you can run the comment lines - the data is genereted by this code.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sources I used: https://www.pyimagesearch.com/2016/08/08/k-nn-classifier-for-image-classification\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import cv2\n",
    "import imutils\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import os\n",
    "os.getcwd()\n",
    "import numpy as np\n",
    "from plotly.subplots import make_subplots\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "import plotly.graph_objects as go\n",
    "#rootdir = r'E:\\Surface\\ML and Apllication in ECO\\Final\\Archive\\training\\training'\n",
    "#val_root = r'E:\\Surface\\ML and Apllication in ECO\\Final\\Archive\\validation\\validation'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This functio create two python dict that match artist to country and country to artist in \n",
    "order for data processing later\"\"\"\n",
    "def dict_createion():\n",
    "    photo_dict = {}\n",
    "    artists = [dI for dI in os.listdir(rootdir) if os.path.isdir(os.path.join(rootdir, dI))]\n",
    "    artist_dict = dict((ar, []) for ar in artists)\n",
    "\n",
    "    for subdir, dirs, files in os.walk(rootdir):\n",
    "        for file in files:\n",
    "            artist = (os.path.basename(os.path.normpath(subdir)))\n",
    "            photo_dict[file] = artist\n",
    "            artist_dict[artist].append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CC - Gilad Green - IML ##\n",
    "def plot_principal_component(pca, i):\n",
    "    # Get PC representation as a subspace with size proportional to the corresponding singular value\n",
    "    size = np.sqrt(pca.singular_values_[i])\n",
    "    pc = np.outer(pca.components_[i], np.array([-1,1])) * size\n",
    "\n",
    "    return go.Scatter3d(x=pc[0], y=pc[1], z=pc[2], mode=\"lines\", opacity=.5,\n",
    "                        line=dict(color=color_scheme[i], width=2*size), name='PC {}'.format(i+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This function loads data that created in the function \n",
    "ImageLoop given in the bottom of the notebook\n",
    "\n",
    "X_train.npy - is table of 3988 rows, row for each smaple\\image\\picture given\n",
    "in kaggle folder \"train\" and 30000 features  - 100*100*3\n",
    "\n",
    "y_train - is table of 3988 rows - represent the artist of each picture (corrspond to X_train)\n",
    "\n",
    "y_country - is table of 3988 rows - represent the artist origin by country (correspod to X_train)\n",
    "\n",
    "all other tables are created in the same manner \n",
    "val created from the val folder given by kaggle while tets is randomly spliting from test\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X = np.load(\"X_train.npy\")\n",
    "y = np.load(\"y_train.npy\")\n",
    "y_country = np.load(\"y_country.npy\")\n",
    "\n",
    "X_val = np.load(\"X_val.npy\")\n",
    "y_val = np.load(\"y_val.npy\")\n",
    "y_country_val = np.load(\"X_train.npy\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CC - Gilad Green - IML ##\n",
    "import plotly.express as px\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "color_scheme = [\"rgb(189,6,96)\", \"rgb(6,189,99)\", \"rgb(6,96,189)\"]\n",
    "\n",
    "pca_model = PCA(n_components=3).fit(X)\n",
    "X_projected = pca_model.transform(X)\n",
    "\n",
    "def ORG_PCA(X,y):\n",
    "#     fig = go.Figure(data = [go.Scatter3d(x = X_projected[:, 0], y=X_projected[:, 1], z=X_projected[:, 2], opacity = 0.75, \n",
    "#                                    mode = 'markers', marker=dict(size=2, color=y), showlegend=False)],\n",
    "\n",
    "    fig = px.scatter_3d(x=X_projected[:, 0], y=X_projected[:, 1], z=X_projected[:, 2], color = y ,size_max=1)\n",
    "    \n",
    "    fig.update_layout(scene_aspectmode=\"cube\",title=r\"$\\text{(1) Projection Onto PCA Subspace}$\",\n",
    "                                 scene_xaxis_title=\"PC1\",\n",
    "                                 scene_yaxis_title=\"PC2\",\n",
    "                                 scene_zaxis_title=\"PC3\")\n",
    "    \n",
    "    fig.update_traces(marker=dict(size=3,\n",
    "                              line=dict(width=2,\n",
    "                                        color='DarkSlateGrey')),\n",
    "                  selector=dict(mode='markers'))\n",
    "    \n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PCA_eval():\n",
    "    ev = pca_model.singular_values_ ** 2\n",
    "    df = pd.DataFrame(np.array([ev, ev / sum(ev), pca_model.explained_variance_ratio_]),\n",
    "              columns=[\"PC 1\", \"PC 2\", \"PC3\"],\n",
    "              index=[\"Eigenvalues\", \"Explained Variance\", \"sklearn's Explained Variance\"])\n",
    "\n",
    "    sumi = np.around(np.sum(pca_model.explained_variance_ratio_),decimals=2)\n",
    "    \n",
    "    variance = list(100 * np.around(pca_model.explained_variance_ratio_,decimals= 2)) + [100*sumi]\n",
    "    fig = make_subplots(rows=1, cols=2,\n",
    "                        subplot_titles=[r\"$\\text{Eigenvalues}$\", r\"$\\text{Cumulative Explained Variance}$\"],\n",
    "                        specs=[[{'type': 'Bar'}, {'type': 'Waterfall'}]])\n",
    "\n",
    "    fig.add_traces([go.Bar(x=['PC1', 'PC2', 'PC3'], y=pca_model.singular_values_, marker_color=color_scheme),\n",
    "                    go.Waterfall(x=[\"PC1\", \"PC2\", \"PC3\", \"Total\"],\n",
    "                                 y=variance,\n",
    "                                 text=[f\"{v}%\" for v in variance],\n",
    "                                 textposition=\"outside\",\n",
    "                                 totals={\"marker\": {\"color\": \"black\"}},\n",
    "                                 measure=[\"relative\", \"relative\", \"relative\", \"total\"])],\n",
    "                   rows=[1, 1], cols=[1, 2])\n",
    "\n",
    "    fig.add_shape(type=\"rect\", xref=\"x\", yref=\"y\", x0=-0.4, x1=0.4, y0=0.0, y1=fig.data[1].y[0],\n",
    "                  fillcolor=color_scheme[0], line=dict(color=color_scheme[0]), opacity=1, row=1, col=2)\n",
    "    fig.add_shape(type=\"rect\", xref=\"x\", yref=\"y\", x0=0.6, x1=1.4, y0=fig.data[1].y[0],\n",
    "                  y1=fig.data[1].y[0] + fig.data[1].y[1],\n",
    "                  fillcolor=color_scheme[1], line=dict(color=color_scheme[1]), opacity=1, row=1, col=2)\n",
    "    fig.add_shape(type=\"rect\", xref=\"x\", yref=\"y\", x0=1.6, x1=2.4, y0=fig.data[1].y[0] + fig.data[1].y[1],\n",
    "                  y1=fig.data[1].y[0] + fig.data[1].y[1] + fig.data[1].y[2],\n",
    "                  fillcolor=color_scheme[2], line=dict(color=color_scheme[2]), opacity=1, row=1, col=2)\n",
    "\n",
    "    fig.update_layout(showlegend=False, title=r\"$\\text{(2) PCA Explained Variance}$\", margin=dict(t=100))\n",
    "    fig.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "PCA invesigation\n",
    "\n",
    "We know we can't really use so much features, but although we reduce the dimention for sure\n",
    "we first wand to evalute - how this reduction effect data? how it's look like?\n",
    "\n",
    "In the following graph (1 - Projection Onto PCA Subspace) we can see each image in 3d dimension -describes as dot for each image - describes as PC1, PC2 and PC3 linear combination.\n",
    "\n",
    "Moreover - each dot color is presenter by the artist origin. Although it's preety - we can't understand much by now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ORG_PCA(X,y_country)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should think and evalute out new sub-space - how good those PC? in the manner of - how well projection over it described the data well (how much of the variance it keeps?)\n",
    "\n",
    "As we learned and discussed in class the proporation between the eigenvalue and sum of all eigenvalue of the features matrix - gives as the proporatinal variance its explaine. We can see the absolute value of the eigenvalue and the proporation in the second graph - (2) PCA explained variacne. \n",
    "\n",
    "In out data our top 3 PC explain in total 45% of data variance, that's cool! instead of 100*100*3 features that describe 100% of pic - we need only 3 to decsribe 45% of variance.\n",
    "\n",
    "In the next graph we evalute PCA reduction for the next mission - cheking out how many feature to use for data classification, when plotting is not needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PCA_eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So next we need do decide - how much number of componenets to use?\n",
    "Well, we know that adding componenet will make analyze harder, so we want to reduce the number of components but on the other hand to keep explained variance high as we can.\n",
    "\n",
    "The following code check component number from 1 to 200 in jump of 5, \n",
    "As we can see 100 components give us 80% variance explain and 200 components give us only 85% variance explain - It's mean that adding 100 components add only 5% variance explanation.\n",
    "\n",
    "So - it the next steps we will analyze the data using only 100 componenets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "cmp = list(range(1,200,5))\n",
    "var_explained = [0.32252795374875093, 0.5473577325010345, 0.6090863546620551, 0.6474343496783221, 0.673818456998201, 0.6933456294213315, 0.7092786280719136, 0.7224780051644024, 0.7337657229416796, 0.743654156105565, 0.7523683178156403, 0.7600835412925967, 0.7670892067807604, 0.7734635045939114, 0.7793024587087803, 0.7847118175294681, 0.7897868575616117, 0.7945211201723869, 0.7990880405200035, 0.8032899778113859, 0.8072921529337342, 0.811031614930826, 0.8145794223657139, 0.8179886879670624, 0.8212279646507146, 0.8242397497509685, 0.8272007624386497, 0.8300208615509651, 0.8327522757355716, 0.8353723555409102, 0.8378770394806213, 0.8403368064905449, 0.8427143753393262, 0.845006529344865, 0.8472000978232513, 0.8493613216003854, 0.8514427063930443, 0.8534826349548814, 0.8554147167894187, 0.8573499438408055]\n",
    "\n",
    "\n",
    "# var_explained = []\n",
    "# for i in cmp:\n",
    "#     pca_model = PCA(n_components=i).fit(X_train)\n",
    "#     var_explained.append(sum(pca_model.explained_variance_ratio_))\n",
    "\n",
    "    \n",
    "\n",
    "fig = go.Figure([ \n",
    "    go.Scatter(name='Variance Explained in PCA model', x=cmp, y=var_explained, mode='markers+lines', marker_color='rgb(152,171,150)')\n",
    "]).update_layout(title=r\"$\\text{(3) Explained Variance as function of number of components in PCA Model}$\", \n",
    "                 xaxis_title=r\"$n \\text{ - Number of Components}$\", \n",
    "                 yaxis_title=r\"$\\text{Variance Explained}$\").show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, what's next -  next we will use the top two components in order to cluster the data using KNN!\n",
    "\n",
    "Due to the fact KNN is a-parametric model, we will use whole X and whole y (and not train, test and validation as we will use in the next mission)\n",
    "\n",
    "we will do that with diffrent neighbers counting - in order to get the best score and to get sense of how the number of neighbers change the clustering map. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Clustring - KNN ## \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn import neighbors, datasets\n",
    "from sklearn import preprocessing\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "neighbors = [2,5,7,10,15,20]\n",
    "\n",
    "le_dict= {'French':0, 'US':1, 'Dutch':2}\n",
    "\n",
    "y_tag = [le_dict[x] for x in y_country]\n",
    "\n",
    "h = 2\n",
    "\n",
    "\n",
    "for n in neighbors:\n",
    "    cmap_light = ListedColormap(['orange', 'cyan', 'cornflowerblue'])\n",
    "    cmap_bold = ['darkorange', 'c', 'darkblue']\n",
    "\n",
    "    proj = X_projected[:,:2]\n",
    "    model = KNeighborsClassifier(n_neighbors=n, weights = 'distance')\n",
    "    model.fit(proj, y_tag)\n",
    "    x_min, x_max = proj[:, 0].min() - 1, proj[:, 0].max() + 1\n",
    "    y_min, y_max = proj[:, 1].min() - 1, proj[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),np.arange(y_min, y_max, h))\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.contourf(xx, yy, Z, cmap=cmap_light)\n",
    "\n",
    "    plt.xlim(xx.min(), xx.max())\n",
    "    plt.ylim(yy.min(), yy.max())\n",
    "\n",
    "    fre_patch = mpatches.Patch(color='orange', label='French')\n",
    "\n",
    "    US_patch = mpatches.Patch(color='cyan', label='US')\n",
    "\n",
    "    Dutch_patch = mpatches.Patch(color='cornflowerblue', label='Dutch')\n",
    "    plt.legend(handles=[Dutch_patch,US_patch,fre_patch])\n",
    "    plt.title(\"KNN using \" + str(n) + \" Neighbers\")\n",
    "\n",
    "plt.show()\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i> So - what can we learn from above clustering?  </i>\n",
    "\n",
    "First it's nice to see the Bias - Variance Trade Off,\n",
    "We can see that increaseing the number of neighbers make the Dutch classification less sagnificant (Bias raise, variance decresed),\n",
    "while when we use low number of neighbers became more significant. \n",
    "\n",
    "This shows the bias - variance tradeoff for KNN due to the fact that using more neighbers increse bias and decreade variance and the opposite dot less neighbers.\n",
    "\n",
    "We also see that french artist rule the centre of PC1 and PC2 whie US rule the edges - mostly the right-up cornet and left up corner.\n",
    "\n",
    "We can see The dutch classification isn't clearly seperated of the French and US, we can point that we don't have many Dutch artists, actually we have only one, which might be not enough for this kind ot mission.\n",
    "\n",
    "Morover I tried to figure out if we can classify artist by their dominant color,\n",
    "I tried to sum up the first 1/3 values of the weight in the component first and second vector - in order to get the \"weight\" over the red color, and to sum up the next 1/3 values to get the weight for the green and blue values (next 1/3 for green and next 1/3 for blue)\n",
    "\n",
    "we can see that there is no most dominant color (at least for two major component we use), unfortunately, it doesn't helped much! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Sum of Red weight in PCA first componet: \",sum(pca_model.components_[0][:1000])),2\n",
    "print(\"Sum of Green weight in PCA first componet: \",sum(pca_model.components_[0][1000:2000]))\n",
    "print(\"Sum of Blue weight in PCA first componet: \",sum(pca_model.components_[0][2000:3000]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second Mission - Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part we will use features matrix we get from projection over top 400 componenets of PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_model = PCA(n_components=400).fit(X)\n",
    "\n",
    "X_train_projected = pca_model.transform(X_train)\n",
    "X_test_projected = pca_model.transform(X_test)\n",
    "X_val_projected = pca_model.transform(X_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Build two classification models in order to predict the painter from the paint. The goal here is to make a good prediction. Please include explanations on the process of developing your models. Be as clear and descriptive as you can be.</b>\n",
    "\n",
    "So, I thought it could be nice just trying multiple models for classification and check out which is the best,\n",
    "By now I will just try to get some sense of how it's look like and next I will choose two of the models and \n",
    "using validation data set I will try to imrove them.\n",
    "So I tried several models that I searched for in - https://scikit-learn.org/stable/supervised_learning.html,\n",
    "my attempts can be viewed in the last section of this notebook\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I - Random Forest Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Model Descripation: </b>\n",
    "\n",
    "Random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. \n",
    "The sub-sample size is controlled with the max_samples parameter if bootstrap=True (default), otherwise the whole dataset is used to build each tree.\n",
    "\n",
    "I choose this model because trees calssifier tend to \"break\" down the data in a greedy way that maximize score over train set,  I thought I could be usefull and fit image processing. \n",
    "\n",
    "Moreover I thought that due to the fact random forest use kind of \"bagging\" in a way it make de-correlation over data while growing the tree (it's choose the k cordinate randomly) could help us with this specific data- seems to by very correlated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Present the tuning process. Alongside your description, add a table with hyper-parameters and their corresponding accuracies on the training and CV datasets, ordered by the CV accuracy in a decreasing order. \n",
    "Show only the best 15 combination; that is, the table should consist 15 rows max.</b>\n",
    "\n",
    "In the table above we can see all the tuning parameters I mentioned and both train, test and validatio score.\n",
    "We should pay attention that we \"cropped\" the table - and kept only 15 lined with best validation.\n",
    "\n",
    "We can see that there is several combination gives best prediction over validation data set. for those combination we can point on kind of trade - off between tree -depth and number of estimators\tand min samples split in the manner that we can get the best result also by increasing tree depth (from 9 to 17/19) and also incresing the number of estimators\tand min samples split.\n",
    "It makes sense because this parameters have opposite effect on the bias-variance trade off, so in total it keeps the result quite the same.\n",
    "\n",
    "So In order to evalute the preformenct of the Random Forest Classifier I foucs first on tuning several of hyper- parameters,\n",
    "I created a table of all combination of all hyper parameter and for evry combination checked the train, validation and test score. The parameter I choose to to use in the tuning process are those I find that can be tuned in the sklearn documantaion can be found here: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
    "\n",
    "1. max depth - The maximum depth of the tree. We know this affect bias-variacne Trade-off in trees high depth -> low bias and high variance.\n",
    "\n",
    "2. number of estimators - The number of trees in the forest.\n",
    "\n",
    "3. min samples split - The minimum number of samples required to split an internal node.\n",
    "\n",
    "4. min samples leaf - The minimum number of samples required to be at a leaf node. A split point at any depth will only be considered if it leaves at least min_samples_leaf training samples in each of the left and right branches. This may have the effect of smoothing the model, especially in regression.\n",
    "\n",
    "5. max samples - If bootstrap is True, the number of samples to draw from X to train each base estimator.\n",
    "\n",
    "\n",
    "I choose this parameters because this are the main parameter of the model that can get diffrent values - and we can try tune them. For every one of the parameters the range I used considered also the default value by sklearn package. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "depth = list(range(1,20,2))\n",
    "nesti = list(range(10,300,20))\n",
    "mss = list(range(2,100,20))\n",
    "msl = list(range(1,100,20))\n",
    "\n",
    "\n",
    "train_errors = []\n",
    "val_errors = []\n",
    "test_errors = []\n",
    "\n",
    "\"\"\"Code generated Pickle: \"\"\"\n",
    "# df = pd.DataFrame(columns=['tree depth', 'number of estimators', 'min samples split',\n",
    "#                           'min samples leaf','train score', 'validation score', 'test score'])\n",
    "\n",
    "\n",
    "# for x in itertools.product(depth, nesti,mss,msl):\n",
    "#     model =RandomForestClassifier(max_depth=x[0],n_estimators=x[1],min_samples_split=x[2], min_samples_leaf =x[3],random_state=42)\n",
    "#     model.fit(X_train_projected,y_train)\n",
    "#     df = df.append({'tree depth': x[0], 'number of estimators':x[1], 'min samples split': x[2],\n",
    "#                    'min samples leaf': x[3],'train score': model.score(X_train_projected, y_train),\n",
    "#                   'validation score':model.score(X_val_projected, y_val),'test score':model.score(X_test_projected, y_test)}, ignore_index=True)\n",
    "\n",
    "# df = df.sort_values(by='validation score', ascending=False)\n",
    "# df.to_pickle(\"Random_forest.pkl\")\n",
    "\n",
    "df = pd.read_pickle(\"Random_forest.pkl\")\n",
    "df =df.sort_values(by='validation score', ascending=False)\n",
    "df.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Evaluate your performance using the tools from the class.</b>\n",
    "\n",
    "We will evaliute our classification using ROC curve,\n",
    "Pay attention it's a bit more complex using multiple classes.\n",
    "\n",
    "https://towardsdatascience.com/understanding-auc-roc-curve-68b2303cc9c5\n",
    "\n",
    "We can learn from the result that over *Test* That although we get poor results for both classifiation and test,\n",
    "using TPR and FPR methods \n",
    "\n",
    "- FPR - Rate of images have not got the artists label although it's belong to artist \n",
    "- TPR - Rate of images got the artists label, and it does belong to artist.\n",
    "\n",
    "But, pay attention to big diffrent in the ROC curve compare to next evalutaion I show,\n",
    "ROC uses the <b>Probability</b> to get specific label - it's not \"binary\", it count also how much out algorithm is \"close\"\n",
    "to true value. For Example - it might be that we predict image wrong, because the wrong artist got higher probability, but it was only a bit more then the correct artists probability, so we want to take this \"close\" to correct answer in acount!\n",
    "\n",
    "So - In manner of ROC curve, we aren't that bad! (Although from the binary point of view we are...)\n",
    "Because there is many lines (ROC curve for each artist - we will use AUC parameter - because this parameter measure of the ability of a classifier to distinguish between classes (here - diffrent artists)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn import datasets, metrics\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from itertools import cycle\n",
    "\n",
    "artirst = ['Cezanne','Degas','Gauguin','Hassam','Matisse','Monet','Pissarro','Renoir','Sargent','VanGogh']\n",
    "\n",
    "model =RandomForestClassifier(max_depth=9,n_estimators=130,min_samples_split=2, min_samples_leaf =21,random_state=42)\n",
    "\n",
    "model.fit(X_train_projected,y_train)\n",
    "\n",
    "y_pred = model.predict(X_test_projected)\n",
    "y_score = model.predict_proba(X_test_projected)\n",
    "\n",
    "y_test_bin = label_binarize(y_test, classes=artirst)\n",
    "n_classes = y_test_bin.shape[1]\n",
    "\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "\n",
    "colors = cycle(['aqua', 'darkorange', 'cornflowerblue'])\n",
    "for i,color in zip(range(n_classes), colors):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], y_score[:, i])\n",
    "    plt.plot(fpr[i], tpr[i], color=color, lw=2)\n",
    "    print('AUC for Class {}: {}'.format(artirst[i], auc(fpr[i], tpr[i])))\n",
    "\n",
    "    \n",
    "#    fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], y_score[:, i])\n",
    "#   plt.plot(fpr[i], tpr[i], color='darkorange', lw=2)\n",
    "#   print('AUC for Class {}: {}'.format(i+1, auc(fpr[i], tpr[i])))\n",
    "\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves - Random Forest')\n",
    "plt.rcParams[\"figure.figsize\"] = (15,10)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Explore your predictions. Which paintings were misclassified? Why?</b>\n",
    "\n",
    "Load X_test and y_test and test your model. -- I talked with David and used splitting train folder given by kaggle into test and train insted.\n",
    "\n",
    "In order to answer that question I will use the best model given by the table above, \n",
    "using:\n",
    "\n",
    "- tree depth = 9\n",
    "- number of estimators = 130\n",
    "- min samples split = 2.0\t\n",
    "- min samples leaf = 21\n",
    "\n",
    "We can see that the artist that the model had the best classification over their artwork are Matisse, Monet and Renoir.\n",
    "The artist we had low sucess in classification over it are Degas,Hassam and Cezanne.\n",
    "\n",
    "It's intresting to understand why those specific artist are missclassified, In order to do that we can search after the tree greedy steps in order to see what happend and in which final \"box\" the artists appear. This will be very hard,so another option that will get the same effect ist to check the Probability to get one of the low misclassificatied artits and to get sense why the algorithm \"confused\" about it. This what I will try to do next (I gave only one of the artists exapmle- because it's similar idea for everyone else)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = {'Cezanne': 0, 'Degas': 0, 'Gauguin': 0,\n",
    "                           'Hassam': 0, 'Matisse': 0, 'Monet': 0,\n",
    "                           'Pissarro': 0, 'Renoir': 0, 'Sargent': 0,\n",
    "                           'VanGogh': 0}\n",
    "score = {'Cezanne': 0, 'Degas': 0, 'Gauguin': 0,\n",
    "                           'Hassam': 0, 'Matisse': 0, 'Monet': 0,\n",
    "                           'Pissarro': 0, 'Renoir': 0, 'Sargent': 0,\n",
    "                           'VanGogh': 0}\n",
    "\n",
    "for i in range(len(y_test)):\n",
    "    if y_pred[i] == y_test[i]:\n",
    "        score[y_test[i]]+=1\n",
    "    total[y_test[i]] +=1\n",
    "\n",
    "for key in total:\n",
    "    print (\"Ratio of Success over tets for: \" + str(key) + \" \"+ str(np.round(score[key]/total[key]*100,2)) +\"%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Degas_index = np.where(y_train == 'Degas')\n",
    "X_train_degas = X_train_projected[Degas_index]\n",
    "degas_pred = model.predict_proba(X_train_degas)\n",
    "sum_per_artists = np.around(degas_pred.sum(axis=0)/len(degas_pred)*100,2)\n",
    "\n",
    "i=0\n",
    "for key in total:\n",
    "    print(\"Probability to classify as \" + str(key) + \" while it's Degas artwork \" + str(sum_per_artists[i]) +\"%\")\n",
    "    i+=1\n",
    "    "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We can see, for example that although degas get the highest rate to classify for it's artwork(14%) It's not really that clear,\n",
    "and it's very close to the chance to get his art work as Renoir or Gauguin who get (11 and 10% respectily)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model II - SVC "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Model Descripation: </b>\n",
    "Generlly, SVM is an  algorithm creates a line or a hyperplane which separates the data into classes.\n",
    "The C parameter tells the SVM optimization how much you want to avoid misclassifying each training example. For large values of C, the optimization will choose a smaller-margin hyperplane if that hyperplane does a better job of getting all the training points classified correctly. Conversely, a very small value of C will cause the optimizer to look for a larger-margin separating hyperplane, even if that hyperplane misclassifies more points. For very tiny values of C, you should get misclassified examples, often even if your training data is linearly separable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Present the tuning process. Alongside your description, add a table with hyper-parameters and their corresponding accuracies on the training and CV datasets, ordered by the CV accuracy in a decreasing order. \n",
    "Show only the best 15 combination; that is, the table should consist 15 rows max.</b>\n",
    "\n",
    "Just as before - we tried to improve classification by tuning several parameters, This are the non-boolean parameters of the model\n",
    "\n",
    "- C - Regularization parameter. The strength of the regularization is inversely proportional to C. Must be strictly positive. The penalty is a squared l2 penalty.\n",
    "\n",
    "- kernel - Specifies the kernel type to be used in the algorithm.\n",
    "\n",
    "- degree - Degree of the polynomial kernel function (‘poly’). Ignored by all other kernels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "## QDA AND LDA NOT POSSIBLE DUE TO Variables are collinear\n",
    "# C = [1,2,3]\n",
    "# kernel = [\"linear\", \"poly\", \"rbf\", \"sigmoid\"]\n",
    "# degree = [1,2,3,4,5,6,7,8,9,10]\n",
    "\n",
    "# df = pd.DataFrame(columns=['C','kernel','degree','train score', 'validation score', 'test score'])\n",
    "# for x in itertools.product(C, kernel,degree):\n",
    "#     model = make_pipeline(StandardScaler(), SVC(gamma='auto', C=x[0],kernel=x[1],degree=x[2]))\n",
    "#     model.fit(X_train_projected,y_train)\n",
    "#     df = df.append({\"C\": x[0],\"kernel\": x[1],\"degree\": x[2], 'train score': model.score(X_train_projected, y_train),\n",
    "#                       'validation score':model.score(X_val_projected, y_val),'test score':model.score(X_test_projected, y_test)}, ignore_index=True)\n",
    "\n",
    "# df = df.sort_values(by='validation score', ascending=False)\n",
    "# df.to_pickle(\"SVC.pkl\")\n",
    "df = pd.read_pickle(\"SVC.pkl\")\n",
    "df.head(15)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Evaluate your performance using the tools from the class.</b>\n",
    "\n",
    "Again we wil evaluter preformenct with ROC curve, we can see the values are quite similar to those of random forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn import datasets, metrics\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from itertools import cycle\n",
    "\n",
    "artirst = ['Cezanne','Degas','Gauguin','Hassam','Matisse','Monet','Pissarro','Renoir','Sargent','VanGogh']\n",
    "\n",
    "model = make_pipeline(StandardScaler(), SVC(gamma='auto',probability=True))\n",
    "\n",
    "model.fit(X_train_projected,y_train)\n",
    "\n",
    "y_pred = model.predict(X_test_projected)\n",
    "y_score = model.predict_proba(X_test_projected)\n",
    "\n",
    "y_test_bin = label_binarize(y_test, classes=artirst)\n",
    "n_classes = y_test_bin.shape[1]\n",
    "\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "\n",
    "colors = cycle(['aqua', 'darkorange', 'cornflowerblue'])\n",
    "for i,color in zip(range(n_classes), colors):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], y_score[:, i])\n",
    "    plt.plot(fpr[i], tpr[i], color=color, lw=2)\n",
    "    print('AUC for Class {}: {}'.format(artirst[i], auc(fpr[i], tpr[i])))\n",
    "\n",
    "    \n",
    "#    fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], y_score[:, i])\n",
    "#   plt.plot(fpr[i], tpr[i], color='darkorange', lw=2)\n",
    "#   print('AUC for Class {}: {}'.format(i+1, auc(fpr[i], tpr[i])))\n",
    "\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves - SVM')\n",
    "plt.rcParams[\"figure.figsize\"] = (15,10)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Explore your predictions. Which paintings were misclassified? Why?</b>\n",
    "\n",
    "same as before - now only for prediction made by the SVM model.\n",
    "\n",
    "Top artists score classification:\n",
    "- Matisse \n",
    "- Monet \n",
    "- Pissarro \n",
    "\n",
    "Top artists score classification:\n",
    "- Cezanne \n",
    "- VanGogh \n",
    "- Degas\n",
    "\n",
    "We alredy saw, that the reson of that misclassification is similarity between diffrent artist works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = {'Cezanne': 0, 'Degas': 0, 'Gauguin': 0,\n",
    "                           'Hassam': 0, 'Matisse': 0, 'Monet': 0,\n",
    "                           'Pissarro': 0, 'Renoir': 0, 'Sargent': 0,\n",
    "                           'VanGogh': 0}\n",
    "score = {'Cezanne': 0, 'Degas': 0, 'Gauguin': 0,\n",
    "                           'Hassam': 0, 'Matisse': 0, 'Monet': 0,\n",
    "                           'Pissarro': 0, 'Renoir': 0, 'Sargent': 0,\n",
    "                           'VanGogh': 0}\n",
    "\n",
    "for i in range(len(y_test)):\n",
    "    if y_pred[i] == y_test[i]:\n",
    "        score[y_test[i]]+=1\n",
    "    total[y_test[i]] +=1\n",
    "\n",
    "for key in total:\n",
    "    print (\"Ratio of Success over tets for: \" + str(key) + \" \"+ str(np.round(score[key]/total[key]*100,2)) +\"%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Discuss the differences between your models, in their assumptions, and explain why did you choose them.\n",
    "Consider manipulating your data to see if it helps you achieve better results.</b>\n",
    "\n",
    "I used also this source - https://datascience.stackexchange.com/questions/6838/when-to-use-random-forest-over-svm-and-vice-versa\n",
    "\n",
    "We alredy talked about each one of the model sepretly, described it and showed model paramerter tuning effect over data.\n",
    "For example, we talked about the bias-varince tradeoff and the paramertes affect this in each of the models (mainly tree depth in random forest and C regularizaion parameter in SVM)\n",
    "We also saw that the models are the same in the way they \"view\"  the problem - due to two of them belong to the field of Supervised learning.\n",
    "I choosed this models because first, while trying many other models from SKLEARN - they did the best,\n",
    "Second, this models are \"rich\" of paramerter can be scaled - so in this kind of problem, I though it would be good.\n",
    "Moreover we might be not that suprised this models worked because, in some manner - they quite similar, and both of them \"split\" the subspace created by feature into \"boxes\" (Tree) or sub-subspaces(SVM).\n",
    "I tried couple of ideas in order to manupulate data -such as not scaling data, using grey-scale histogram or red, blue green histogram splitted, but, none of this ideas really works (example of histogram I used given in the bottom of the notebook)\n",
    "I guess that people who are expert in Image processing could have beteer ideas based on more complex theory\n",
    "\n",
    "SVM VS RANDOM FOREST -  \n",
    "\n",
    "| Criteria | SVM | Random Forest |\n",
    "| :-------- | :-------- | :-------- |\n",
    "| Fits multiclass problems? | No, we get probability from distance calculation | Yes |\n",
    "| Data scaling? | Need | No need |\n",
    "| Complexity? | High, due to n x n matrix | Low |\n",
    "| When to use? | problem might not be linearly separable | handle large number of training example,handle non-linear data  |\n",
    "| What's common | Supervised | Supervised  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>How does model performance? Discuss.</b>\n",
    "\n",
    "For both model we sae low results for validation and tests set,\n",
    "But if we investigate the result a bit more we can see, using ROC curve that in manner of probability - the classification isn't that bad, and although it's wrong in \"botoom line\" - it's quite close to corrct results.\n",
    "\n",
    "Unfortunalty, We reached only  ~31% rate of succee for validation and test sets in, both random forest ans SVM.\n",
    "\n",
    "Because I tried many other models, eventhough it's not greate, it's better then other.\n",
    "\n",
    "I think the main reson the models aren't very good is that is that some of artwork are quite the same - or very similar.\n",
    "Also, I think that maybe using thet data \"as is\" with no manipulation (beside resize, that this step only re-scale it but don't change it) maybe cause this low rate of success.\n",
    "Moreover I believe that maybe high level of applications in Deep Learning, that build specifly in order to image processing and classification can help us imporve Results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function I used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Other model trying\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "alphas=[1e-4,1e-3, 1e-2, 1e-1, 1]\n",
    "\n",
    "# df = pd.DataFrame(columns=['alpha','train score', 'validation score', 'test score'])\n",
    "\n",
    "# for a in alphas:\n",
    "#     model =RidgeClassifier(alpha=a)\n",
    "#     model.fit(X_train_projected,y_train)\n",
    "#     df = df.append({'alpha': a,'train score': model.score(X_train_projected, y_train),\n",
    "#                   'validation score':model.score(X_val_projected, y_val),'test score':model.score(X_test_projected, y_test)}, ignore_index=True)\n",
    "\n",
    "# df = df.sort_values(by='validation score', ascending=False)\n",
    "# df.to_pickle(\"RidgeClassifier.pkl\")\n",
    "df = pd.read_pickle(\"RidgeClassifier.pkl\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The function used to resize Data to fixed form od 100*100*4\n",
    "\"\"\"\n",
    "def resize():\n",
    "    for subdir, dirs, files in os.walk(rootdir):\n",
    "        for file in files:\n",
    "\n",
    "            # print os.path.join(subdir, file)\n",
    "            filepath = os.path.join(subdir, file)\n",
    "\n",
    "            if os.path.isfile(filepath):\n",
    "                im = Image.open(filepath)\n",
    "                imResize = im.resize((100, 100), Image.ANTIALIAS)\n",
    "                rgb_im = imResize.convert('RGB')\n",
    "                rgb_im.save(filepath, \"JPEG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This functin used to create feature matrix and labels matrix for train and validation data \n",
    "given by kaggle\n",
    "\n",
    "Pay attention test was created by diving train data (in trian folder of kaggle int)\n",
    "\"\"\"\n",
    "def imageLoop():\n",
    "    labels_dict_country = {'Cezanne': 'French', 'Degas': 'French', 'Gauguin': 'French',\n",
    "                           'Hassam': 'US', 'Matisse': 'French', 'Monet': 'French',\n",
    "                           'Pissarro': 'French', 'Renoir': 'French', 'Sargent': 'US',\n",
    "                           'VanGogh': 'Dutch'}\n",
    "\n",
    "    for subdir, dirs, files in os.walk(rootdir):\n",
    "        for file in files:\n",
    "            filepath = os.path.join(subdir, file)\n",
    "            if os.path.isfile(filepath):\n",
    "                artist = (os.path.basename(os.path.normpath(subdir)))\n",
    "                label = labels_dict_country[artist]\n",
    "                image = cv2.imread(filepath)\n",
    "                pixel = image.flatten()\n",
    "                # hist = extract_color_histogram(image)\n",
    "                rawImages.append(pixel)\n",
    "                features.append(pixel / 256)\n",
    "                # another optin was features.append(hist) = gave much less\n",
    "                artists_labels.append(artist)\n",
    "                labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Nice graph for random forest\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "depth = list(range(1,20,2))\n",
    "\n",
    "train_errors = []\n",
    "val_errors = []\n",
    "test_errors = []\n",
    "\n",
    "for d in depth:\n",
    "    model =RandomForestClassifier(max_depth=d, random_state=42)\n",
    "    model.fit(X_train_projected,y_train)\n",
    "    \n",
    "    train_errors.append(model.score(X_train_projected, y_train))\n",
    "    val_errors.append(model.score(X_val_projected, y_val))\n",
    "    test_errors.append(model.score(X_test_projected, y_test))\n",
    "    \n",
    "    \n",
    "fig = go.Figure([ \n",
    "    go.Scatter(name='Train Score', x=depth, y=train_errors, mode='markers+lines', marker_color='rgb(152,171,150)'),\n",
    "    go.Scatter(name='Validation Score', x=depth, y=val_errors, mode='markers+lines', marker_color='rgb(220,179,144)'),\n",
    "    go.Scatter(name='Test Score', x=depth, y=test_errors, mode='markers+lines', marker_color='rgb(25,115,132)')]).update_layout(title=r\"$\\text{Random Forest Score over Train, Validation and Test Sets}$\", \n",
    "                 xaxis_title=r\"$D\\text{ - Tree Depth}$\", \n",
    "                 yaxis_title=r\"$\\text{Score Value}$\").show() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_color_histogram(img, bins=(8, 8, 8)):\n",
    "    # extract a 3D color histogram from the HSV color space using\n",
    "    # the supplied number of `bins` per channel\n",
    "    #     hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "    #     hist = cv2.calcHist([hsv], [0, 1, 2], None, bins,\n",
    "    #                         [0, 180, 0, 256, 0, 256])\n",
    "    #     print(hist.shape)\n",
    "    #     # handle normalizing the histogram if we are using OpenCV 2.4.X\n",
    "    #     if imutils.is_cv2():\n",
    "    #         hist = cv2.normalize(hist)\n",
    "    #     # otherwise, perform \"in place\" normalization in OpenCV 3 (I\n",
    "    #     # personally hate the way this is done\n",
    "    #     else:\n",
    "    #         cv2.normalize(hist, hist)\n",
    "\n",
    "    color = ('b', 'g', 'r')\n",
    "    hist = []\n",
    "    for i, col in enumerate(color):\n",
    "        histr = cv2.calcHist([img], [i], None, [256], [0, 256])\n",
    "        hist.append(histr.flatten())\n",
    "\n",
    "    arr = np.array(hist).T.flatten()\n",
    "\n",
    "    # return the flattened histogram as the feature vector\n",
    "    #     print(hist.flatten().shape)\n",
    "    #     return hist.flatten()\n",
    "    return arr"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
